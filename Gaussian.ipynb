{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ECE763': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c0164b6fc447729148a9dbbb776f9cf8796208f9f8eb26f117ee183b2f428654"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ECE 763 Project 01: Face Detection\n",
    "## faceScrub dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "### Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(actors_list):\n",
    "  img_set = [] # empty list\n",
    "  for actor in tqdm(actors_list, total=len(actors_list) ,desc='Loading dataset'):\n",
    "    img0 = {} # empty dictionary\n",
    "    img1 = {} # empty dictionary\n",
    "    try:\n",
    "      # create dictionary and add it to the list\n",
    "      img0['file'] = actor['0']\n",
    "      img0['class'] = 0\n",
    "      img0['image'] = Image.open(actor['0'])\n",
    "      img_set.append(img0)\n",
    "      img1['file'] = actor['1']\n",
    "      img1['class'] = 1\n",
    "      img1['image'] = Image.open(actor['1'])\n",
    "      img_set.append(img1)\n",
    "    except:\n",
    "      print(actor['name'] + \" couldn't be found!\")\n",
    "  return img_set"
   ]
  },
  {
   "source": [
    "Load datasets into memmory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:00<00:00, 2796.82it/s]\n",
      "Loading dataset: 100%|██████████| 1000/1000 [00:00<00:00, 4712.04it/s]\n"
     ]
    }
   ],
   "source": [
    "test_actors = []\n",
    "train_actors = []\n",
    "\n",
    "test_file = './data/test/test.txt'\n",
    "train_file = './data/training/training.txt'\n",
    "\n",
    "# Read test data\n",
    "with open(test_file, newline='') as actors:\n",
    "    actors_reader = csv.DictReader(actors, delimiter='\\t')\n",
    "    for actor in actors_reader:\n",
    "        test_actors.append(actor)\n",
    "\n",
    "# Read training data\n",
    "with open(train_file, newline='') as actors:\n",
    "    actors_reader = csv.DictReader(actors, delimiter='\\t')\n",
    "    for actor in actors_reader:\n",
    "        train_actors.append(actor)\n",
    "\n",
    "# Build datasets\n",
    "test_set = buildDataset(test_actors)\n",
    "train_set = buildDataset(train_actors)"
   ]
  },
  {
   "source": [
    "Concatenate RGB values and normalize data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample from test set: \n[0.4        0.42745098 0.63137255 ... 0.24313725 0.28235294 0.41960784]\nshape:  (1200,)\nSample from training set: \n[0.41568627 0.21960784 0.17647059 ... 0.4        0.25098039 0.2       ]\nshape:  (1200,)\n"
     ]
    }
   ],
   "source": [
    "# Convert images to np array\n",
    "for face in test_set:\n",
    "    face['image'] = np.asfarray(face['image']).flatten() / 255.0\n",
    "for face in train_set:\n",
    "    face['image'] = np.asfarray(face['image']).flatten() / 255.0\n",
    "\n",
    "# Inspect data\n",
    "print('Sample from test set: ')\n",
    "print(test_set[0]['image'])\n",
    "print('shape: ' , test_set[0]['image'].shape)\n",
    "print('Sample from training set: ')\n",
    "print(train_set[0]['image'])\n",
    "print('shape: ' , test_set[0]['image'].shape)"
   ]
  },
  {
   "source": [
    "Crete training class subsets and X matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X0 shape:  (1200, 1000)\nX1 shape:  (1200, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Create vectors of training data\n",
    "X0_subset = [ face for face in train_set if face['class']==0 ]\n",
    "X1_subset = [ face for face in train_set if face['class']==1 ]\n",
    "x_len = train_set[0]['image'].shape[0]\n",
    "\n",
    "X0 = np.empty( (x_len,len(X0_subset)) )\n",
    "X1 = np.empty( (x_len,len(X1_subset)) )\n",
    "\n",
    "for i,face in enumerate(X0_subset):\n",
    "    try:\n",
    "        X0[:,i] = face['image']\n",
    "    except:\n",
    "        print(i, face['image'].shape, face['file'])\n",
    "\n",
    "for i,face in enumerate(X1_subset):\n",
    "    X1[:,i] = face['image']\n",
    "\n",
    "# Sanity check\n",
    "print('X0 shape: ' , X0.shape)\n",
    "print('X1 shape: ' , X1.shape)"
   ]
  },
  {
   "source": [
    "Compute sample mean vector and sample covariance matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "s0 aprox sigma0? True\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "mu0 = X0.mean(axis=1)\n",
    "mu1 = X1.mean(axis=1)\n",
    "\n",
    "# Variance\n",
    "sigma0 = np.cov(X0, bias=True)\n",
    "sigma1 = np.cov(X1, bias=True)\n",
    "\n",
    "s0 = np.zeros( (X0.shape[0],X0.shape[0]) )\n",
    "for i in range(X0.shape[1]):\n",
    "    try:\n",
    "        d = X0[:,i][:, np.newaxis] - mu0[:, np.newaxis]\n",
    "    except:\n",
    "        print(i)\n",
    "    dm = (d @ d.T)\n",
    "    s0 = s0 + dm\n",
    "s0 /= X0.shape[1]\n",
    "print('s0 aprox sigma0?' , np.allclose(s0,sigma0))\n",
    "\n"
   ]
  }
 ]
}