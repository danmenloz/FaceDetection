{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ECE763': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c0164b6fc447729148a9dbbb776f9cf8796208f9f8eb26f117ee183b2f428654"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ECE 763 Project 01: Face Detection\n",
    "## faceScrub dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "### Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(actors_list):\n",
    "  img_set = [] # empty list\n",
    "  for actor in tqdm(actors_list, total=len(actors_list) ,desc='Loading dataset'):\n",
    "    img0 = {} # empty dictionary\n",
    "    img1 = {} # empty dictionary\n",
    "    try:\n",
    "      # create dictionary and add it to the list\n",
    "      img0['file'] = actor['0']\n",
    "      img0['class'] = Image.open(actor['0'])\n",
    "      img_set.append(img0)\n",
    "      img1['file'] = actor['1']\n",
    "      img1['class'] = Image.open(actor['1'])\n",
    "      img_set.append(img1)\n",
    "    except:\n",
    "      print(actor['name'] + \" couldn't be found!\")\n",
    "  return img_set"
   ]
  },
  {
   "source": [
    "Load datasets into memmory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading dataset: 100%|██████████| 100/100 [00:00<00:00, 7156.66it/s]\n",
      "Loading dataset: 100%|██████████| 1000/1000 [00:00<00:00, 12392.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_actors = []\n",
    "train_actors = []\n",
    "\n",
    "test_file = './data/test/test.txt'\n",
    "train_file = './data/training/training.txt'\n",
    "\n",
    "with open(test_file, newline='') as actors:\n",
    "    actors_reader = csv.DictReader(actors, delimiter='\\t')\n",
    "    for actor in actors_reader:\n",
    "        test_actors.append(actor)\n",
    "\n",
    "with open(train_file, newline='') as actors:\n",
    "    actors_reader = csv.DictReader(actors, delimiter='\\t')\n",
    "    for actor in actors_reader:\n",
    "        train_actors.append(actor)\n",
    "\n",
    "test_set = buildDataset(test_actors)\n",
    "train_set = buildDataset(train_actors)"
   ]
  }
 ]
}